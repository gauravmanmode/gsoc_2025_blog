{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6c5f98",
   "metadata": {},
   "source": [
    "# GSoC 2025 with Optimagic: Adding More Optimizer Interfaces to Optimagic\n",
    "---\n",
    "\n",
    "## Intro\n",
    "\n",
    "``` python\n",
    "import optimagic as om\n",
    "```\n",
    "\n",
    "This is a blog for the Google Summer of Code 2024 project, entitled ['Adding More Optimizer Interfaces to Optimagic'](https://summerofcode.withgoogle.com/programs/2024/projects/), under the [NumFOCUS](https://numfocus.org/) organization, on the [Optimagic Project](https://estimagic.org/), which is supported by NumFOCUS.\n",
    "\n",
    "## Implementation decisions\n",
    "\n",
    "## PyTrees\n",
    "\n",
    "## Code Contributions\n",
    "While the primary objective was adding more optimizers to optimagic a number of additional changes. These changes are outlined below in order.\n",
    "\n",
    "## Optimizers from Nevergrad[(Merged)](https://github.com/optimagic-dev/optimagic/pull/591)\n",
    "\n",
    "### Why Nevergrad?\n",
    "\n",
    "Nevergrad\n",
    "### Covariance Matrix Adaptation Evolution Strategy\n",
    "This is also a good one\n",
    "\n",
    "### OnePlusOne Evolution Strategy\n",
    "\n",
    "### Random Search\n",
    "\n",
    "### SamplingS earch\n",
    "\n",
    "### Differential Evolution\n",
    "\n",
    "### Bayesian Optimization\n",
    "\n",
    "### Estimation of Distribution Algorithm\n",
    "\n",
    "### Test-Based Population Sampling Adaptaion\n",
    "\n",
    "### Estimation of Multivariate Normal Algorithm (EMNA)\n",
    "\n",
    "### NGOPT Optimizers\n",
    "\n",
    "### META Optimizers\n",
    "\n",
    "### Example usage\n",
    "\n",
    "```python\n",
    "import optimagic as om\n",
    "results = []\n",
    "for algo in om.algorithms.AVAILABLE_ALGORITHMS.values():\n",
    "    if algo.startswith(\"nevergrad\"):\n",
    "        res = om.minimize(\n",
    "            fun = lambda x: x@x,\n",
    "            algorithm = algo\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "\n",
    "fig = om.criterion_plot(\n",
    "    results,\n",
    "    max_evaluations=180,\n",
    "    monotone=True,\n",
    ")\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "### Adding `needs_bounds` and `supports_infinite_bounds` fields in theÂ AlgoInfo [(Merged)](https://github.com/optimagic-dev/optimagic/pull/610)\n",
    "\n",
    "While all global optimizers run with bounds, optimizers from nevergrad can run without bounds where bounds are implicitly set to .\n",
    "Hence, we introduced two new fields in the AlgoInfo class, namely `needs_bounds` and `supports_infinite_bounds`.\n",
    "\n",
    "```python\n",
    "from optimagic.algorithms import AVAILABLE_ALGORITHMS\n",
    "\n",
    "algos_with_bounds_support = [\n",
    "    algo\n",
    "    for name, algo in AVAILABLE_ALGORITHMS.items()\n",
    "    if algo.algo_info.supports_bounds\n",
    "]\n",
    "my_selection = [\n",
    "    algo for algo in algos_with_bounds_support if algo.algo_info.needs_bounds\n",
    "]\n",
    "my_selection[0:3]\n",
    "```\n",
    "\n",
    "```python\n",
    "my_selection2 = [\n",
    "    algo\n",
    "    for algo in algos_with_bounds_support\n",
    "    if algo.algo_info.supports_infinite_bounds\n",
    "]\n",
    "my_selection2[0:3]\n",
    "```\n",
    "\n",
    "## Migrate nevergrad optimizers to new documentation style [(Merged)](https://github.com/optimagic-dev/optimagic/pull/632)\n",
    "\n",
    "\n",
    "\n",
    "## Wrap Local Optimizers from Gradient Free Optimizers [(Open)](https://github.com/optimagic-dev/optimagic/pull/624)\n",
    "\n",
    "### Hill Climbing\n",
    "### Stochastic Hill Climbing\n",
    "### Simulated Annealing\n",
    "### Repulsing Hill Climbing\n",
    "### Downhill Simplex Optimization\n",
    "### Simulated Annealing\n",
    "### Powell's Method\n",
    "\n",
    "## Wrap Population Based Optimizers from Gradient Free Optimizers [(Open)](https://github.com/optimagic-dev/optimagic/pull/636)\n",
    "\n",
    "### Particle Swarm Optimization\n",
    "### Spiral Optimization\n",
    "### Genetic Algorithm\n",
    "### Evolution Strategy\n",
    "### Differential Evolution\n",
    "\n",
    "### Rework `test_many_algorithms`\n",
    "\n",
    "\n",
    "### New Example in class SphereExampleInternalOptimizationProblemWithConverter\n",
    "\n",
    "We introduced a new example in the `internal_optimization_problem.py` which could be used for testing with PyTrees.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from optimagic.optimization.internal_optimization_problem import SphereExampleInternalOptimizationProblemWithConverter\n",
    "from optimagic.typing import AggregationLevel\n",
    "problem = SphereExampleInternalOptimizationProblemWithConverter(solver_type=AggregationLevel.LEAST_SQUARES)\n",
    "problem.converter.params_to_internal({\"x0\":2,\"x1\":3})\n",
    "problem.converter.derivative_to_internal(\n",
    "    {\n",
    "    \"x0\":{\n",
    "         \"x0\":2,\n",
    "         \"x1\":3,\n",
    "        },\n",
    "   \"x1\":{\n",
    "       \"x0\":2,\n",
    "       \"x1\":3\n",
    "       }\n",
    "    },\n",
    "    [2,])\n",
    "```\n",
    "\n",
    "## Add L-BFGS optimizer from pyensmallen [(Open)](https://github.com/optimagic-dev/optimagic/pull/566)\n",
    "\n",
    "ensmallen is a very fast library in C++ for cheap objective functions. This has been pending because of development from the pyensmallen repo.\n",
    "\n",
    "\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I would like to express my gratitude to the following individuals and institutions for their support and contributions:\n",
    "\n",
    "Firstly I would like to my mentors of the GSOC project, for their kind welcome to the community and receptivity to new ideas, as well as for fostering a generally constructive and engaging atmosphere for discussion.\n",
    "[Janos Gabler](https://janosg.github.io/) and [Tim Mensinger](https://janosg.github.io/) for aware of the possibilty of a GSOC project.\n",
    "\n",
    "Furthermore, I would be remiss if I did not acknowledge the contributions of all community members who provided assistance through comments and feedback on various pull requests.\n",
    "\n",
    "In conclusion, I would like to express my gratitude to the Google Summer of Code program for providing me with the opportunity and financial support, which enabled me to pursue my academic interests and enhance my technical abilities with minimal constraints."
   ]
  }
 ],
 "metadata": {
  "file_format": "mystnb",
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  },
  "source_map": [
   5
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}